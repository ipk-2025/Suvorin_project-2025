# **Гибридная система временной верификации (v11): Архитектура и компоненты**

Финальная версия системы (v11) представляет собой гибридную модель, которая достигла пиковой точности **87.0%** и средней точности **76.3%** в задаче бинарной классификации текстов по двум временным периодам: "ранний" (1904-1905) и "поздний" (1906-1908).

Ключевой особенностью архитектуры стал отказ от нестабильного дообучения (fine-tuning) нейронных сетей в пользу более надежного подхода, основанного на извлечении признаков и использовании мощного классического алгоритма — градиентного бустинга.

## **Архитектура системы**

Система состоит из трех основных модулей, которые последовательно обрабатывают данные:

1. **Модуль извлечения признаков (Feature Extractor)**: Преобразует каждый исходный текст в числовой вектор, комбинируя семантическую и стилометрическую информацию.  
2. **Модуль отбора и масштабирования (Selector & Scaler)**: Оптимизирует набор стилометрических признаков и приводит все данные к единому масштабу.  
3. **Модуль классификации (Classifier)**: Принимает на вход готовые векторы и обучает модель для предсказания временного периода.

Ниже представлено подробное описание каждого компонента.

### **1\. Постановка задачи: Бинарная классификация**

Первоначальная задача по разделению текстов на 4 класса (по годам) оказалась слишком сложной для текущего объема данных. Ключевым стратегическим решением стало ее упрощение:

* **Класс 0:** "Ранний период" (тексты за 1904 и 1905 годы).  
* **Класс 1:** "Поздний период" (тексты за 1906 и 1908 годы).

Это позволило модели сфокусироваться на более выраженных стилистических изменениях, которые могли произойти под влиянием социальных и политических событий 1905 года.

### **2\. Модуль извлечения признаков**

Этот модуль является ядром системы и отвечает за создание богатого векторного представления для каждого документа. Он состоит из двух параллельных потоков.

#### **2.1. Извлечение семантических признаков (BERT)**

* **Модель:** Используется предобученная модель-трансформер DeepPavlov/rubert-base-cased, натренированная на огромном корпусе русскоязычных текстов.  
* **Метод использования:** Модель **не дообучается**, а используется в режиме **экстрактора признаков**. Это позволяет избежать переобучения на малом датасете и получить стабильные, семантически насыщенные представления текстов.  
* **Стратегия пулинга: Mean Pooling (Усреднение)**  
  * В отличие от стандартного подхода с использованием эмбеддинга одного \[CLS\] токена, здесь применяется более продвинутый метод.  
  * **Как это работает:** Для каждого текста модель генерирует эмбеддинги для всех его токенов. Затем эти векторы эмбеддингов усредняются (с учетом маски внимания, чтобы игнорировать "пустые" \[PAD\] токены).  
  * **Результат:** На выходе для каждого документа получается один 768-мерный вектор, который комплексно отражает семантику всего текста, а не только информацию, агрегированную в \[CLS\] токене.

#### **2.2. Извлечение стилометрических и синтаксических признаков**

Параллельно с BERT извлекается большой набор "классических" признаков, которые характеризуют авторский стиль. Они делятся на несколько групп:

* **Лексические признаки:** Средняя длина слова, коэффициент лексического разнообразия (Type-Token Ratio).  
* **Частотность пунктуации:** Частота использования основных знаков препинания (., ,, ;, :, \!, ?).  
* **Морфологические признаки:** Частота использования различных частей речи (существительные, глаголы, прилагательные, наречия и т.д.), полученных с помощью pymorphy2.  
* **N-граммы символов:** Частота встречаемости самых популярных 3-грамм и 4-грамм символов. Эти признаки улавливают уникальные для автора подстроки и слоговые конструкции.  
* **N-граммы частей речи:** **Ключевое нововведение**, захватывающее синтаксическую структуру текста. Анализируется частота самых популярных последовательностей из 2 и 3 частей речи (например, "прилагательное \-\> существительное" или "существительное \-\> глагол \-\> наречие").

Всего генерируется более 2500 таких признаков.

### **3\. Модуль отбора и масштабирования**

* **Отбор признаков (SelectKBest):** Из 2500+ стилометрических признаков отбираются **200 самых информативных** с точки зрения статистики (ANOVA F-test). Это позволяет убрать "шумные" признаки и оставить только те, которые лучше всего коррелируют с целевыми классами.  
* **Объединение:** Отобранные 200 стилометрических признаков объединяются с 768 семантическими признаками от BERT. В итоге каждый документ представляется в виде единого вектора размерностью **968** (768 \+ 200).  
* **Масштабирование (StandardScaler):** Все признаки в итоговом векторе приводятся к единому масштабу (с нулевым средним и единичной дисперсией). Это обязательный шаг для корректной работы большинства алгоритмов машинного обучения, включая LightGBM.

### **4\. Модуль классификации (LightGBM)**

На финальном этапе используется мощный алгоритм градиентного бустинга.

* **Модель:** lgb.LGBMClassifier.  
* **Причина выбора:** LightGBM был выбран вместо RandomForest и нейросетевых классификаторов, так как он демонстрирует высочайшую производительность на табличных данных, устойчив к переобучению и очень быстр.  
* **Гиперпараметры:** Используются настроенные параметры, найденные в ходе предыдущих экспериментов, для обеспечения баланса между сложностью модели и регуляризацией (n\_estimators=600, learning\_rate=0.02, num\_leaves=41 и др.).

### **5\. Стратегия обучения и оценки: Ансамблирование**

Для получения максимально надежной и стабильной оценки качества системы применяется ансамблирование:

* **Количество запусков (n\_runs):** Вся процедура кросс-валидации повторяется 3 раза.  
* **Случайное разделение:** Каждый запуск использует свой собственный random\_state, что гарантирует разное разделение данных на обучающую и валидационную выборки.  
* **Кросс-валидация (StratifiedKFold):** Внутри каждого запуска используется стратифицированная 5-блочная кросс-валидация. Это обеспечивает сохранение исходного соотношения классов в каждой подвыборке.

**Итоговый результат** вычисляется как усреднение всех метрик по всем фолдам всех запусков, что дает очень робастную оценку производительности модели, минимизируя влияние случайности.
